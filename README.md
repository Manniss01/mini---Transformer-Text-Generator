# ğŸ§  Mini-Transformer Text Generator

A custom-trained Transformer-based language model that generates Shakespearean-style text. Built with modern attention mechanisms and designed for experimentation, creativity, and educational exploration.

---

## ğŸš€ Project Overview

This repository contains a minimalist Transformer implementation for autoregressive text generation. Leveraging a compact architecture trained on Shakespearean text, it allows you to explore how modern attention mechanisms generate coherent, stylistic text.

### Key Highlights:
- End-to-end Transformer model built from scratch in PyTorch
- Adjustable decoding strategies to balance creativity and coherence
- Interactive web interface powered by Gradio for real-time experimentation
- Modular codebase designed for easy retraining and extension to other corpora

---

## ğŸ”§ Features

- **Transformer Architecture:** Multi-head self-attention, positional embeddings, and feedforward networks
- **Custom Sampling Controls:**
  - ğŸ”¥ Temperature: Adjust randomness  
  - ğŸ”¢ Top-k sampling: Restrict next-token choices to top-k probabilities  
  - ğŸ§® Top-p (nucleus) sampling: Probabilistically trim tail tokens  
  - ğŸ² Sampling toggle: Switch between stochastic and greedy decoding  
- **Efficient Training Pipeline:** Batch processing, periodic evaluation, and loss monitoring
- **User-Friendly UI:** Seamless text generation with live parameter tuning via Gradio

---

## ğŸ—‚ï¸ Project Structure
mini-transformer-text-generator/
â”œâ”€â”€ app.py # Gradio web app for text generation
â”œâ”€â”€ config.py # Configuration and hyperparameters
â”œâ”€â”€ model.py # Transformer model implementation
â”œâ”€â”€ train.py # Training script
â”œâ”€â”€ utils.py # Utilities for data processing and evaluation
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ input.txt # Training data (Shakespeare text)
â””â”€â”€ README.md # Project documentation

## âš™ï¸ Model Configuration

| Parameter           | Value |
|--------------------|-------|
| Batch size          | 32    |
| Context length      | 128   |
| Embedding size      | 128   |
| Transformer layers  | 4     |
| Attention heads     | 4     |
| Dropout             | 0.1   |
| Learning rate       | 0.001 |

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/yourusername/mini-transformer-text-generator.git
cd mini-transformer-text-generator
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

python app.py
```


# Technologies
- Python 3.8+
- PyTorch for deep learning
- Gradio for web interface and demo
- Numpy and standard Python libraries

## About Unicodax
This project is developed and maintained by Unicodax â€” an AI-first company focused on building intelligent solutions using modern machine learning and NLP technologies.

Visit us: ğŸŒ www.unicodax.com

# Contact
Questions, feedback, or collaborations? Feel free to open an issue or reach out via manishdarji.ai@gmail.com.

# Acknowledgements
Inspired by Andrej Karpathyâ€™s minGPT, which provided foundational guidance on building Transformers from scratch.


